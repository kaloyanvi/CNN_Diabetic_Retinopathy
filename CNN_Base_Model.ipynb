{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Base_Model.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kvidelov/CNN_Diabetic_Retinopathy/blob/master/CNN_Base_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfzMXplgR9_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC8H3S6_KHAH",
        "colab_type": "text"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBnRS5K-JfWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import itertools\n",
        "import multiprocessing.pool\n",
        "import threading\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "import keras\n",
        "import pandas as pd\n",
        "from keras import backend as K\n",
        "from keras import layers, models\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.backend import relu, sigmoid\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import utils\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
        "from tensorflow.contrib.session_bundle import exporter\n",
        "import os\n",
        "from skimage import exposure\n",
        "\n",
        "\n",
        "def model_fn(labels_dim):\n",
        "    \"\"\"Create a Keras Sequential model with layers.\"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     border_mode=\"valid\",\n",
        "                     input_shape=(128, 128, 3)))\n",
        "    \n",
        "  \n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(labels_dim, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "    compile_model(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def compile_model(model):\n",
        "    #opt = keras.optimizers.Adagrad(lr=0.01)\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=\"adadelta\",\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def read_train_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Train Data\")\n",
        "    data = np.load(\"trainDataSmall.npz\")\n",
        "    #data = np.load(\"/content/drive/My Drive/Colab Notebooks/trainDataSmall.npz\")\n",
        "    print(\"Train data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_train = data[\"X_train\"]\n",
        "    Y_train = data[\"Y_train\"]\n",
        "    print(\"Training - Total examples per class\", np.sum(Y_train, axis=0))\n",
        "    return [X_train, Y_train]\n",
        "\n",
        "\n",
        "def read_test_data():\n",
        "    start_time = time.time()\n",
        "    print(\"Start Read Test Data\")\n",
        "    data = np.load(\"testDataSmall.npz\")\n",
        "    #data = np.load(\"/content/drive/My Drive/Colab Notebooks/testDataSmall.npz\")\n",
        "    print(\"Test data read --- %s seconds ---\" % (time.time() - start_time))\n",
        "    X_test = data[\"X_test\"]\n",
        "    Y_test = data[\"Y_test\"]\n",
        "    print(\"Testing - Total examples per class\", np.sum(Y_test, axis=0))\n",
        "    return [X_test, Y_test]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5JvWnOPKBzh",
        "colab_type": "text"
      },
      "source": [
        "# Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSuYY-albXsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This code implements a Feed forward neural network using Keras API.\"\"\"\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "CLASS_SIZE = 5\n",
        "FILE_PATH = 'checkpoint.{epoch:02d}.hdf5'\n",
        "RETINOPATHY_MODEL = 'retinopathy.hdf5'\n",
        "\n",
        "class ContinuousEval(keras.callbacks.Callback):\n",
        "    \"\"\"Continuous eval callback to evaluate the checkpoint once\n",
        "       every so many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 eval_frequency,\n",
        "                 job_dir):\n",
        "        self.eval_frequency = eval_frequency\n",
        "        self.job_dir = job_dir\n",
        "        [self.X_test, self.Y_test] = read_test_data()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        if epoch > 0 and epoch % self.eval_frequency == 0:\n",
        "            # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "            # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "            model_path_glob = 'checkpoint.*'\n",
        "            model_path_glob = os.path.join(self.job_dir, model_path_glob)\n",
        "            checkpoints = glob.glob(model_path_glob)\n",
        "            if len(checkpoints) > 0:\n",
        "                checkpoints.sort()\n",
        "                retinopathy_model = load_model(checkpoints[-1])\n",
        "                retinopathy_model = compile_model(retinopathy_model)\n",
        "                loss, acc = retinopathy_model.evaluate(\n",
        "                    self.X_test, self.Y_test)\n",
        "                print('\\nEvaluation epoch[{}] metrics[{:.2f}, {:.2f}] {}'.format(\n",
        "                    epoch, loss, acc, retinopathy_model.metrics_names))\n",
        "            else:\n",
        "                print('\\nEvaluation epoch[{}] (no checkpoints found)'.format(epoch))\n",
        "      \n",
        "def AHE(image):\n",
        "      image = exposure.equalize_adapthist(image, clip_limit=0.01)\n",
        "      return image\n",
        "\n",
        "\n",
        "def run():\n",
        "    tf.keras.backend.clear_session()\n",
        "    #local dir to write checkpoints and export model\n",
        "    job_dir = 'jobdir'\n",
        "    #Batch size for training steps\n",
        "    train_batch_size = 200\n",
        "    #Maximum number of epochs on which to train\n",
        "    num_epochs = 50\n",
        "    #Checkpoint per n training epochs\n",
        "    checkpoint_epochs = 5\n",
        "    #Perform one evaluation per n epochs\n",
        "    eval_frequency = 5\n",
        "\n",
        "    retinopathy_model = model_fn(CLASS_SIZE)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(job_dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Unhappy hack to work around h5py not being able to write to GCS.\n",
        "    # Force snapshots and saves to local filesystem, then copy them over to GCS.\n",
        "    checkpoint_path = FILE_PATH\n",
        "    checkpoint_path = os.path.join(job_dir, checkpoint_path)\n",
        "\n",
        "    # Model checkpoint callback\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_loss',\n",
        "        verbose=2,\n",
        "        period=checkpoint_epochs,\n",
        "        mode='max')\n",
        "\n",
        "    # Continuous eval callback\n",
        "    evaluation = ContinuousEval(eval_frequency,\n",
        "                                job_dir)\n",
        "\n",
        "    # Tensorboard logs callback\n",
        "    tblog = keras.callbacks.TensorBoard(\n",
        "        log_dir=os.path.join(job_dir, 'logs'),\n",
        "        histogram_freq=0,\n",
        "        write_graph=True,\n",
        "        embeddings_freq=0)\n",
        "\n",
        "    callbacks = [checkpoint, evaluation, tblog]\n",
        "\n",
        "    [X_train, Y_train] = read_train_data()\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        width_shift_range = 0.1,\n",
        "        height_shift_range = 0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "        preprocessing_function=AHE)\n",
        "    \n",
        "    val_datagen = ImageDataGenerator()\n",
        "    validation_generator = val_datagen.flow(evaluation.X_test, evaluation.Y_test, batch_size=train_batch_size)\n",
        "\n",
        "    history = retinopathy_model.fit_generator(\n",
        "        train_datagen.flow(X_train, Y_train, batch_size=train_batch_size),\n",
        "        steps_per_epoch=100,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=2,\n",
        "        validation_data= validation_generator)\n",
        "\n",
        "    retinopathy_model.save(os.path.join(job_dir, RETINOPATHY_MODEL))\n",
        "    \n",
        "    # Creating a plot for loss and accuracy for both training and validation set.\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    #print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'], 'bo')\n",
        "    plt.plot(history.history['val_accuracy'], 'b')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_acc', 'test_acc'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'], 'bo')\n",
        "    plt.plot(history.history['val_loss'], 'b')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_loss', 'test_loss'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfhm4mqBJ2wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive') ## If you want to read from your drive - this is not required, as you can get the files as below, but it might faster\n",
        "\n",
        "## this will download the data from the internet, it may fail if the server is not up\n",
        "![ -f testDataSmall.npz ] || wget -O testDataSmall.npz \"https://www.win.tue.nl/~cdecampos/testDataSmall.npz\"\n",
        "![ -f trainDataSmall.npz ] || wget -O trainDataSmall.npz \"https://www.win.tue.nl/~cdecampos/trainDataSmall.npz\"\n",
        "!rm -fr jobdir/\n",
        "\n",
        "run()\n",
        "#drive.flush_and_unmount() ## if you need to unmount your google drive"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}